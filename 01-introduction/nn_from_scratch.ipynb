{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-based Neural Network engine from scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/short-dl/blob/master/01-introduction/nn_from_scratch.ipynb)\n",
    "\n",
    "\n",
    "В этой тетрадке мы напишем маленький фреймворк для создания и обучения нейронной сети.\n",
    "\n",
    "**Цели тетрадки**\n",
    "\n",
    "1. Понять, что лежит в основе DL фреймворков\n",
    "2. Разобраться с бекпропом на примере простых слоев\n",
    "3. Встретить характерные для векторизованных вычислений ошибки\n",
    "4. Вспомнить особенности операций с плавающей точкой\n",
    "5. Попробовать тестировать код сразу\n",
    "\n",
    "\n",
    "**План работы**\n",
    "\n",
    "0. Обсудить векторизацию вычислений и договориться о формате входных-выходных данных\n",
    "1. Рассмотреть как высокоуровнево устроен движок для сеток\n",
    "2. Реализовать несколько базовых слоев\n",
    "3. Натренировать простую сеть на задаче MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация вычислений (batchification)\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_vector.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "В современных компьютерах выгодно объединять данные в массивы и применять векторизованные операции.\n",
    "\n",
    "На практике во всех фреймворках для DL работают с данными упакованными в батчи: \n",
    "многомерные массивы с размерами `[batch_size, channels, *spatial_dimensions]` -- такие массивы называются тензорами. \n",
    "\n",
    "Подразумевается, что примеры в батче независимы друг от друга и никак не влияют друг на друга (за исключением некоторых нормализаций и специальных трюков).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_batched.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Предлагается реализовать простой движок для обучения простейших нейронных сетей (MLP, Multi Layer Perceptron).\n",
    "Для этого нам будет достаточно нескольких строительных блоков:\n",
    "\n",
    "- класс для цепочки слоев (Sequential)\n",
    "- классы для слоев (реализации операций)\n",
    "- train loop\n",
    "\n",
    "\n",
    "Для простоты предлагается совместить backward-проход с градиентным спуском.\n",
    "Если слой содержит веса, надо будет их обновить в коде `backward`-метода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот слой можно было бы назвать Identity, он пропускает данные и градиенты без изменений\n",
    "class Layer:\n",
    "    def forward(self, input):\n",
    "        # store something if needed\n",
    "        return input\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        # apply grads here if any\n",
    "        return grads\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        for layer in self.layers[::-1]:\n",
    "            grads = layer.backward(grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сеть которая не делает ничего\n",
    "network = Sequential([Layer(), Layer()])\n",
    "\n",
    "x = np.random.random(size=(3, 5))\n",
    "output = network.forward(x)\n",
    "assert output.shape == (3, 5)\n",
    "\n",
    "y = np.ones((3, 5))\n",
    "grads = network.backward(y)\n",
    "assert grads.shape == (3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть блоки которые преобразуют вектора в вектора. Технически, этого достаточно чтобы решать задачи регрессии, однако классической задачей для MLP считается классификация.\n",
    "\n",
    "Чтобы решать задачи классификации нам не хватает еще нескольких операций\n",
    "\n",
    "## softmax \n",
    "\n",
    "Это функция, которая превращает произвольный вектор в корректное распределение вероятностей (неотрицательные числа, суммирующиеся в 1) $$q_i = \\frac{\\exp(x_i)}{\\sum \\limits_{j} \\exp(x_j)}$$\n",
    "\n",
    "\n",
    "## cross-entropy loss (перекрестная энтропия)\n",
    "\n",
    "$$H(\\mathbf{p}, \\mathbf{q}) = -\\sum \\limits_i p_i \\log q_i$$.\n",
    "\n",
    "Эта формула описывает близость между двумя распределениями вероятностей:\n",
    " - $i$ --- номер класса,\n",
    " - $p_i$ --- правильная вероятность класса (единица, еcли правильная метка i, ноль, если нет)\n",
    " - $q_i$ --- предсказанная вероятность (выход softmax'а).\n",
    " \n",
    "В математическом смысле кросс-энтропия не является расстоянием, в частности потому что меняется при перестановке аргументов.\n",
    "\n",
    "\n",
    "\n",
    "Вычисление экспонент и логарифмов часто ведет к накоплению ошибок, поэтому часто вместо вычисления вероятностей вычисляют логиты ($\\log \\mathrm{softmax}$) и из формулы для лосса убирают логарифмы.\n",
    "\n",
    "При использовании фреймворков обяхательно надо сверяться с документацией, какие аргументы ожидаются на вход. В лучшем случае в коде есть проверки и мы получим исключение, в худшем -- все будет молча плохо работать.\n",
    "Типичное название для такой лосс-функции `cross_entropy_with_logits`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math vs implementation\n",
    "\n",
    "Нас не интересует строгое соответствие математической нотации, она годится для работы с векторами. Мы же работаем с данными собранными в батчи.\n",
    "\n",
    "\n",
    "Для простоты договоримся, что данные лежат в массивах `numpy.ndarray` с размерностями `[bs, dim]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "<img src=\"https://github.com/m12sl/dl-hse-2020/raw/master/01-introduction/img/graph_mlp.png\" crossorigin=\"anonymous\"/>\n",
    "\n",
    "\n",
    "Нам потребуется реализовать следующие слои:\n",
    "\n",
    "```\n",
    "Linear\n",
    "Relu\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "Синонимы: Linear, Dense, FullyConnected, Projection:\n",
    "\n",
    "Математическое действие над вектором признаков:\n",
    "$$\\mathbf{y} = W \\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "Пусть \n",
    "$\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "$\\mathbf{y}\\in \\mathbb{R}^m$\n",
    "\n",
    "**Какую размерность должен иметь $W$ и $b$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь реализуем векторизованный вариант слоя: `y = f(x, W, b)`\n",
    "\n",
    "```python\n",
    "x.shape == (batch_size, in_channels)\n",
    "y.shape == (batch_size, out_channels)\n",
    "```\n",
    "\n",
    "Напишите код для вычисления прямого прохода и обратного в векторизованной форме.\n",
    "Шаг градиентного спуска предлагается совместить с обратным проходом.\n",
    "\n",
    "Веса $W, b$ считаем принадлежащими слою, инициализируем при создании, во время обратного прохода возвращаем градиенты только за входной тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуйте линейный слой\n",
    "# y = Wx + b\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.W = np.random.randn(in_channels, out_channels) * 0.01\n",
    "        self.b = np.zeros(out_channels)\n",
    "        self.learning_rate = learning_rate\n",
    "        # for debug purpose\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "        self.grad = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        <напишите свой код>\n",
    "        return y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        <напишите вычисление градиентов>\n",
    "        \n",
    "        # применение градиентов\n",
    "        self.W = self.W - grad_W * self.learning_rate\n",
    "        self.b = self.b - grad_b * self.learning_rate\n",
    "        \n",
    "        # это может пригодиться для дебага\n",
    "        self.grad_W = grad_W\n",
    "        self.grad_b = grad_b\n",
    "        self.grad = grad\n",
    "        return grad\n",
    "\n",
    "    \n",
    "# тесты для самопроверки\n",
    "linear = Linear(5, 7)\n",
    "\n",
    "x = np.random.random(size=(3, 5))\n",
    "assert linear.forward(x).shape == (3, 7), \"Forward pass shape mistmatch\"\n",
    "\n",
    "y = np.random.random(size=(3, 7))\n",
    "grad_x = linear.backward(y)\n",
    "assert grad_x.shape == (3, 5)\n",
    "assert linear.grad_W.shape == linear.W.shape\n",
    "assert linear.grad_b.shape == linear.b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "\n",
    "Это простая поэлементная операция, записывается как $f(x) = max(x, 0)$.\n",
    "\n",
    "**NB: активационные слои не меняют размер тензора**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        <напишите код>\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        <напишите код>\n",
    "    \n",
    "relu = ReLU()\n",
    "x = np.random.normal(size=(3, 5))\n",
    "assert relu.forward(x).shape == (3, 5)\n",
    "assert np.all(relu.forward(x) >= 0.0)\n",
    "\n",
    "y = np.random.normal(size=(3, 5))\n",
    "grads = relu.backward(y)\n",
    "assert grads.shape == (3, 5) \n",
    "\n",
    "# сверим с численным дифференцированием\n",
    "approx = (relu.forward(x + 1e-5) - relu.forward(x)) / 1e-5\n",
    "\n",
    "relu.forward(x)\n",
    "assert np.allclose(relu.backward(np.ones_like(x)), approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета ошибки нам необходимы предсказания сети и правильные ответы.\n",
    "\n",
    "Для простоты подсчет ошибки и градиентов вынесены в отдельные функции вместе с вычислением вероятностей (softmax).\n",
    "\n",
    "Каждая из функций ожидает на вход логиты -- тензор с размерами `[bs, num_classes]`.\n",
    "\n",
    "`softmax_crossentropy_with_logits` возвращает одно число -- величину ошибки.\n",
    "\n",
    "`grad_softmax_crossentropy_with_logits` возвращает тензор размером `[bs, num_classes]` с градиентами по логитам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch, n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy.mean()\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling\n",
    "Для работы с данными во всех фреймворках есть Dataset/Dataloader классы.\n",
    "\n",
    "Мы будем разбирать pytorch Data API отдельно, а пока предлагается воспользоваться готовым кодом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# в FashionMNIST уже есть код, который грузит данные и читает их в PIL.Image, мы конвертируем их в вектора.\n",
    "# Непосредственно с картинками мы будем работать позже\n",
    "def transform(pil_image):\n",
    "    return np.asarray(pil_image).astype(np.float32).reshape(-1) / 255.0\n",
    "\n",
    "\n",
    "def numpy_collate_fn(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    return np.array(x).astype(np.float32), np.array(y).astype(np.int32)\n",
    "\n",
    "\n",
    "# dataset умеет выдавать отдельные семплы с метками\n",
    "train_dataset = FashionMNIST(\"./tmp\", train=True, download=True, transform=transform)\n",
    "test_dataset = FashionMNIST(\"./tmp\", train=False, download=True, transform=transform)\n",
    "# dataloader выдает целые батчи и поддерживает итерирование\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, collate_fn=numpy_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=32, collate_fn=numpy_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset возвращает отдельные семплы\n",
    "print(train_dataset[11][0].shape, train_dataset[11][1])\n",
    "\n",
    "# dataloader целые батчи\n",
    "for x, y in train_loader:\n",
    "    print(x.dtype, y.dtype)\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"Class: {train_dataset[i][1]}\")\n",
    "    plt.imshow(train_dataset[i][0].reshape(28, 28),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n",
    "\n",
    "Теперь все готово чтобы сделать сеть и проучить ее.\n",
    "Если все реализовано правильно, можно ожидать точности на валидации $~0.86+$.\n",
    "\n",
    "Кросс-энтропийный лосс не слишком показательная величина, но он должен оказаться $~0.4$ на валидации.\n",
    "Если сеть не может вытащить ничего полезного из входа, в результате обучения можно ожидать тривиального ответа по вероятность $0.1$ для каждого класса. Это даст точность $0.1$ и кросс-энтропию порядка $\\log 10 \\approx 2.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential([\n",
    "    Linear(784, 40),\n",
    "    ReLU(), \n",
    "    Linear(40, 40),\n",
    "    ReLU(),\n",
    "    Linear(40, 10),\n",
    "])\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# логи для построения кривых\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in tqdm(train_loader):\n",
    "        logits = network.forward(x)\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        acc = np.mean(np.argmax(logits, axis=-1) == y)\n",
    "        grads = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "        network.backward(grads)\n",
    "        \n",
    "    for x, y in tqdm(test_loader):\n",
    "        logits = network.forward(x)\n",
    "        loss = softmax_crossentropy_with_logits(logits, y)\n",
    "        acc = np.mean(np.argmax(logits, axis=-1) == y)\n",
    "        logs['loss'].append(loss)\n",
    "        logs['acc'].append(acc)\n",
    "        \n",
    "    for k, v in logs.items():\n",
    "        print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## кривые обучения\n",
    "\n",
    "Постройте графики обучения: точность и лосс от количества шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## примеры предсказаний\n",
    "\n",
    "Выведите пример картинок, предсказаний и правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<your code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
